{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Dimensional Example\n",
    "\n",
    "While the previous example demonstrated the basic usage of the scan package,\n",
    "one very important part still needs to be discussed: The hyperparameters of RC.\n",
    "\n",
    "To do so, here we look at a much higher dimensional system than before, the\n",
    "Lorenz-96 (L96) system, another chaotic system that, in constrast to the Lorenz-63\n",
    "system, can be scaled to an arbitrarily large number of dimensions.\n",
    "Here we scale it to be 40 dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "simulation_time_steps = 100000\n",
    "lor_dim = 40\n",
    "lor_dt = 0.05\n",
    "lor_force = 5\n",
    "\n",
    "starting_point = lor_force * np.ones(lor_dim) + 1e-5 * np.random.rand(lor_dim)\n",
    "sim_data = scan.simulations.Lorenz96(sys_dim=lor_dim, force=lor_force, dt=lor_dt).simulate(\n",
    "    time_steps=simulation_time_steps, starting_point=starting_point\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we want to train the system with data sampled from the chaotic\n",
    "attractor we want to predict. As a result,\n",
    "we have to throw the first 10000 simulated data points away, as they are\n",
    "not on said attractor, but instead correspond to transient system dynamics\n",
    "of the L96 system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "signal = sim_data[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional artifact of simulated systems like this, is that they are of\n",
    "course perfectly noiseless. For higher dimensional systems, this can result\n",
    "in RC \"overfitting\" the data, resulting in mostly accurate short term\n",
    "predictions at the cost of many diverging completely after some time.\n",
    "\n",
    "Somewhat counterintuitively, adding a small amount of artificial noise to\n",
    "the training data can not only ensure the long term stability of the\n",
    "prediction, but also decrease the short term prediction error.\n",
    "\n",
    "Note that this procedure depends on the system in question, and is of\n",
    "course not an issue when training with any even slightly noise real world\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(scale=0.01, size=signal.shape)\n",
    "signal = signal + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we again want to create the reservoir/network, but this time the default\n",
    "parameters are not sufficient for RC to learn the system dynamics. For the L96\n",
    "system the following parameters work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "n_dim = 5000  # network dimension\n",
    "n_rad = 0.8  # spectral radius\n",
    "n_avg_deg = 3  # average network degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complicated and higher dimensional the system is, the larger the \n",
    "network n_dim needs to be.  \n",
    "The spectral radius n_rad should be smaller than 1 and must be larger than 0.  \n",
    "The average network degree n_avg_deg can, in theory, be everything in \n",
    "(0, n_dim]. Typically, sparse networks perform better than dense ones though.\n",
    "\n",
    "So far no reliable heuristics exist for what the correct hyperparameter ranges\n",
    "for an arbitrary system might be. Luckily, RC is usually so fast to train and\n",
    "evaluate, that you can go over the entire reasonable range of hyperparameter values\n",
    "with basically any hyperparameter optimization code of your choice, or even just a\n",
    "simple grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random seed, set to generate the same network everytime, is an\n",
    "important hyperparameter too as different random networks can vary hugely in\n",
    "their prediction performance even if all other hyperparameters are the same.\n",
    "Any trustworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the network with those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "esn = scan.ESNWrapper()\n",
    "esn.create_network(n_dim=n_dim, n_rad=n_rad, n_avg_deg=n_avg_deg, n_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train() and train_and_predict() methods too, have a set of hyperparameters one\n",
    "needs to optimize, the most important of which for this system is the regularization\n",
    "parameter which is typically somewhere between 1e-2 and 1e-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "reg_param = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addtionally, RC, like most machine learning methods, works best with normalized data. A mean of 0 and a standard\n",
    "deviation of 1 are the default choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_mean = np.mean(signal)\n",
    "signal_std = np.std(signal)\n",
    "\n",
    "signal_normalized = (signal - signal_mean) / signal_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also specify a nonlinear transformation to be used during the fitting of \n",
    "the output matrix, which vastly decreases number of failed, diverging \n",
    "predictions by breaking up harmful symmetries in the linear RC setup. For how \n",
    "and why see e.g.:  \n",
    "[Herteux, J.; RÃ¤th, C. Breaking Symmetries of the Reservoir Equations in Echo State Networks. Chaos 2020, 30.](\n",
    "https://aip.scitation.org/doi/10.1063/5.0028993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "w_out_fit_flag = \"linear_and_square_r\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of training/prediction time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "train_sync_steps = 1000\n",
    "train_steps = 50000\n",
    "pred_steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train+predict the system using the above hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred, y_test = esn.train_and_predict(\n",
    "    signal_normalized,\n",
    "    train_sync_steps=train_sync_steps,\n",
    "    train_steps=train_steps,\n",
    "    pred_steps=pred_steps,\n",
    "    reg_param=reg_param,\n",
    "    w_out_fit_flag=w_out_fit_flag,\n",
    "    w_in_seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, sharex=\"all\", figsize=(9, 6), constrained_layout=True, dpi=300)\n",
    "\n",
    "vmin = np.min(y_test)\n",
    "vmax = np.max(y_test)\n",
    "\n",
    "im = axs[0].imshow(y_test.T, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"Simulation\")\n",
    "\n",
    "axs[1].imshow(y_pred.T, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"Prediction\")\n",
    "\n",
    "axs[2].imshow(y_pred.T - y_test.T, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "axs[2].set_title(\"Difference between simulation and prediction\")\n",
    "\n",
    "axs[1].set_ylabel(\"dimension\")\n",
    "axs[2].set_xlabel(\"time steps\")\n",
    "fig.colorbar(im, ax=axs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}